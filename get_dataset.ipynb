{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import urllib\n",
    "import csv\n",
    "from tqdm import tqdm\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Modify the cell below with the folder you prefer (training data will be stored in there)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "DATA_ROOT_FOLDER = \"/media/edge7/TOSHIBA EXT\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get COCO annotation file, plus IMDb-face CSV (they are 2 well-known datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-10 17:14:54--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\r\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.216.97, 54.231.224.145, 52.217.85.68, ...\r\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.216.97|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 252907541 (241M) [application/zip]\r\n",
      "Saving to: ‘annotations_trainval2017.zip’\r\n",
      "\r\n",
      "annotations_trainva 100%[===================>] 241.19M  28.4MB/s    in 9.2s    \r\n",
      "\r\n",
      "2023-03-10 17:15:04 (26.4 MB/s) - ‘annotations_trainval2017.zip’ saved [252907541/252907541]\r\n",
      "\r\n",
      "Archive:  annotations_trainval2017.zip\r\n",
      "  inflating: annotations/instances_train2017.json  \r\n",
      "  inflating: annotations/instances_val2017.json  \r\n",
      "  inflating: annotations/captions_train2017.json  \r\n",
      "  inflating: annotations/captions_val2017.json  \r\n",
      "  inflating: annotations/person_keypoints_train2017.json  \r\n",
      "  inflating: annotations/person_keypoints_val2017.json  \r\n",
      "--2023-03-10 17:15:08--  https://storage.googleapis.com/public_stuff/IMDb-Face.csv\r\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.209.48, 142.250.184.80, 142.250.180.176, ...\r\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.209.48|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 291641388 (278M) [text/csv]\r\n",
      "Saving to: ‘IMDb-Face.csv.1’\r\n",
      "\r\n",
      "IMDb-Face.csv.1     100%[===================>] 278.13M  60.1MB/s    in 13s     \r\n",
      "\r\n",
      "2023-03-10 17:15:21 (21.5 MB/s) - ‘IMDb-Face.csv.1’ saved [291641388/291641388]\r\n",
      "\r\n",
      "Creating layout folder in /media/edge7/TOSHIBA EXT\r\n",
      "done\r\n"
     ]
    }
   ],
   "source": [
    "!bash get_annotations.sh \"{DATA_ROOT_FOLDER}\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set Number of Images, feel free to change the below"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "N_PERSON_TOTAL = 5500 # Modify this, as you want\n",
    "N_PERSON_COCO = int(N_PERSON_TOTAL * 0.3)\n",
    "N_PERSON_IMDB = N_PERSON_TOTAL - N_PERSON_COCO\n",
    "N_NOT_PERSON_TOTAL = N_PERSON_TOTAL # Modify this if you don't want a balanced dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.29s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco=COCO('annotations/instances_val2017.json')\n",
    "catIds = coco.getCatIds(catNms=['person'])\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "img_infos = coco.loadImgs(imgIds)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1650/1650 [17:14<00:00,  1.60it/s]\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "with tqdm(total=N_PERSON_COCO) as pbar:\n",
    "    for img_info in img_infos:\n",
    "        img_url = img_info['coco_url']\n",
    "        img_filename = f\"{DATA_ROOT_FOLDER}/data/person/{img_info['file_name']}\"\n",
    "        urllib.request.urlretrieve(img_url, img_filename)\n",
    "        counter +=1\n",
    "        pbar.update(1)\n",
    "        if counter == N_PERSON_COCO:\n",
    "         break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now get the remaining from the IMDb-face dataset, the file should have been automatically downloaded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3850/3850 [18:16<00:00,  3.51it/s]  \n"
     ]
    }
   ],
   "source": [
    "from urllib.error import HTTPError\n",
    "import random\n",
    "seed_value = 1234\n",
    "random.seed(seed_value)\n",
    "\n",
    "counter = 0\n",
    "with open('IMDb-Face.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    with tqdm(total=N_PERSON_IMDB) as pbar:\n",
    "        for row in reader:\n",
    "            if counter == N_PERSON_IMDB:\n",
    "                break\n",
    "            if random.random() < 0.2: # to avoid to take the same person too many times\n",
    "                img_filename = f\"{DATA_ROOT_FOLDER}/data/person/IMDB_{counter}_{row['image']}\"\n",
    "                try:\n",
    "                    urllib.request.urlretrieve(row['url'], img_filename)\n",
    "                    counter +=1\n",
    "                    pbar.update(1)\n",
    "                except HTTPError as e:\n",
    "                    pass #  Some images are not available, just skip\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 3722/5530 [36:42<17:49,  1.69it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "catIds = coco.getCatIds()\n",
    "catIds.remove(coco.getCatIds(catNms=['person'])[0])\n",
    "N = int(N_NOT_PERSON_TOTAL / len(catIds)) +1\n",
    "\n",
    "# get image IDs for all categories except 'person'\n",
    "imgIds = []\n",
    "for catId in catIds:\n",
    "    imgIds.append((coco.loadCats(catId)[0]['name'], coco.getImgIds(catIds=[catId])))\n",
    "\n",
    "\n",
    "# download the images\n",
    "with tqdm(total=N * len(catIds)) as pbar:\n",
    "    for cat, imgs in imgIds:\n",
    "        imgs = coco.loadImgs(imgs)\n",
    "        COUNTER_PER_CATEGORY = 0\n",
    "        for img_info in imgs:\n",
    "            img_url = img_info['coco_url']\n",
    "            img_filename = f\"{DATA_ROOT_FOLDER}/data/notperson/{cat}_{img_info['file_name']}\"\n",
    "            # load the annotations for the image\n",
    "            annIds = coco.getAnnIds(imgIds=img_info['id'])\n",
    "            anns = coco.loadAnns(annIds)\n",
    "            if not any([ann['category_id'] == 1 for ann in anns]): # be 101% sure no person is in the image\n",
    "                pbar.update(1)\n",
    "                try:\n",
    "                    urllib.request.urlretrieve(img_url, img_filename)\n",
    "                    COUNTER_PER_CATEGORY +=1\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                if COUNTER_PER_CATEGORY == N:\n",
    "                    break\n",
    "\n",
    "print(\"DONE!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### It can be that we are missing some images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_images(n):\n",
    "    # Set the API endpoint URL\n",
    "    url = \"https://source.unsplash.com/random\"\n",
    "\n",
    "    # Set the request parameters\n",
    "    params = {\n",
    "        \"orientation\": \"landscape\",\n",
    "        \"content_filter\": \"high\",\n",
    "        \"topics\": [\"nature\", \"food\", \"animals\", \"architecture\", \"travel\", \"art\", \"textures\", \"patterns\"]\n",
    "    }\n",
    "\n",
    "    # Initialize a set to keep track of downloaded image URLs\n",
    "    downloaded_urls = set()\n",
    "\n",
    "    # Loop over N images\n",
    "    for i in range(n):\n",
    "        # Send the request to the API\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "        except Exception:\n",
    "            continue\n",
    "        # Check if the response contains a valid image and it has not been downloaded before\n",
    "        if response.status_code == 200 and response.url not in downloaded_urls:\n",
    "            # Add the downloaded URL to the set of downloaded URLs\n",
    "            downloaded_urls.add(response.url)\n",
    "\n",
    "            # Yield the image content\n",
    "            yield response.content\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "number_of_no_person_images_so_far = len(os.listdir(f\"{DATA_ROOT_FOLDER}/data/notperson/\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "still missing some images, recovering . . . \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1392/1778 [18:10<05:02,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "if number_of_no_person_images_so_far < N_NOT_PERSON_TOTAL:\n",
    "    print(\"still missing some images, recovering . . . \")\n",
    "    number_to_download = N_NOT_PERSON_TOTAL - number_of_no_person_images_so_far\n",
    "    counter = 0\n",
    "    with tqdm(total=number_to_download) as pbar:\n",
    "        for response in download_images(number_to_download):\n",
    "            with open(f\"{DATA_ROOT_FOLDER}/data/notperson/unsplash_{counter}.jpg\", \"wb\") as f:\n",
    "                f.write(response)\n",
    "                pbar.update(1)\n",
    "                counter +=1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done dataset ready for training\n"
     ]
    }
   ],
   "source": [
    "print(\"All done dataset ready for training\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "env",
   "language": "python",
   "display_name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
